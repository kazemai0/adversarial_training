seml:
  name: inductive_soft_median
  executable: experiments/experiment_adv_train_inductive.py
  project_root_dir: ../..
  output_dir: log


slurm:
  experiments_per_job: 1
  sbatch_options:
    gres: gpu:1       # num GPUs
    mem: 32G          # memory
    cpus-per-task: 4  # num cores
    time: 0-20:00     # max time, D-HH:MM 


fixed:
  train_params:
    lr: 1e-2
    weight_decay: 1e-3
    patience: 200
    max_epochs: 3000
  artifact_dir: cache
  model_storage_type: pretrained_ind_adv_final
  device: 0
  data_device: 0
  make_undirected: True
  binary_attr: False
  pretrain_epochs: 10
  validate_every: 1
  attack: LRBCD
  balance_test: True
  train_attack_params:
    epochs: 20
    loss_type: tanhMargin
    lr_factor: 2000
    fine_tune_epochs: 0
    keep_heuristic: WeightOnly
    search_space_size: 1_000_000
    with_early_stopping: False
  val_attack_params:
    epochs: 20
    loss_type: tanhMargin
    lr_factor: 2000
    fine_tune_epochs: 0
    keep_heuristic: WeightOnly
    search_space_size: 1_000_000
    with_early_stopping: False


grid:
  seed:
    type: choice
    options:
     - 0
     - 1
     - 5
  self_training:
    type: choice
    options:
     - False
  robust_epsilon:
    type: choice
    options:
     - 0
  dataset:
    type: choice
    options:
     - cora
     - cora_ml
     - citeseer
    #  - pubmed

soft_median_gdc_t0_2:
  fixed:
    model_params:
      label: Soft Median GDC (T=0.2)
      model: RGNN
      dropout: 0.5
      n_filters: 64
      mean: soft_median
      mean_kwargs:
          temperature: 0.2
      gdc_params:
          alpha: 0.15
          k: 64
      svd_params: None
      jaccard_params: None
      do_cache_adj_prep: True